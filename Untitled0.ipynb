{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere faiss-cpu\n",
        "!pip install langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4POBCmgN0kAs",
        "outputId": "df008e33-ac24-47b8-e248-f9deda26f5d8",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.6.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.34.153-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Collecting botocore<1.35.0,>=1.34.153 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.34.153-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.23.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.153->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.153->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading cohere-5.6.2-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.34.153-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.34.153-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, parameterized, jmespath, httpx-sse, h11, fastavro, faiss-cpu, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.34.153 botocore-1.34.153 cohere-5.6.2 faiss-cpu-1.8.0.post1 fastavro-1.9.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240712\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain)\n",
            "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.27->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.28-py3-none-any.whl (379 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.96-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.12 langchain-core-0.2.28 langchain-text-splitters-0.2.2 langsmith-0.1.96 orjson-3.10.6 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# הגדרת המודל והטוקנייזר של heBERT\n",
        "model_name = \"avichr/heBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# פונקציה ליצירת embeddings לשאלה נתונה\n",
        "def get_embedding(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # אנו לוקחים את הממוצע של כל ה-embeddings של הטוקנים\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXHODrjl05eu",
        "outputId": "9a880c76-c5ba-4c23-898b-9bd71d5ffba3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# קריאת הקובץ JSON\n",
        "with open('/content/sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# שליפת כל השאלות מהקובץ\n",
        "questions = []\n",
        "for book in data['contain'].values():\n",
        "    for chapter in book.values():\n",
        "        for verse in chapter.values():\n",
        "            for qa in verse:\n",
        "                questions.append(qa['question'])\n",
        "\n",
        "# המרת כל השאלות ל-embeddings\n",
        "embeddings = np.array([get_embedding(question) for question in questions])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmnOq_vcqQl",
        "outputId": "ffb252d1-eded-4d39-d7ba-efb742375351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# בניית אינדקס FAISS\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# פונקציה למציאת השאלות הקרובות ביותר\n",
        "def find_nearest_questions(query, k=5):\n",
        "    query_embedding = get_embedding(query).reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
        "    return nearest_questions\n",
        "\n"
      ],
      "metadata": {
        "id": "1BZrDfx4aWBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# שאלה לדוגמה\n",
        "query = \"האם נשים חיבות בתפילה\"\n",
        "\n",
        "# מציאת השאלות הקרובות ביותר\n",
        "nearest_questions = find_nearest_questions(query, k=5)\n",
        "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
        "for q in nearest_questions:\n",
        "    print(q)"
      ],
      "metadata": {
        "id": "PLQxxhyiagLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEPXf9zy0QgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# הגדרת המודל והטוקנייזר של heBERT\n",
        "model_name = \"avichr/heBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# פונקציה ליצירת embeddings\n",
        "def embeddingVectors(text):\n",
        "    encoding = tokenizer.batch_encode_plus(\n",
        "        text,  # List of input texts\n",
        "        padding=True,  # Pad to the maximum sequence length\n",
        "        truncation=True,  # Truncate to the maximum sequence length if necessary\n",
        "        return_tensors='pt',  # Return PyTorch tensors\n",
        "        add_special_tokens=True  # Add special tokens CLS and SEP\n",
        "    )\n",
        "    input_ids = encoding['input_ids']  # Token IDs\n",
        "    attention_mask = encoding['attention_mask']  # Attention mask\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        word_embeddings = outputs.last_hidden_state  # (batch_size, sequence_length, embedding_dim)\n",
        "        # החזרת ה-embedding של ה-CLS token\n",
        "        sentence_embedding = word_embeddings[:, 0, :]  # בחירת האיבר הראשון בכל רצף (CLS token)\n",
        "    return sentence_embedding  # This contains the embeddings\n",
        "\n",
        "# קריאת הקובץ JSON\n",
        "with open('/content/sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# שליפת כל השאלות מהקובץ\n",
        "questions = []\n",
        "for book in data['contain'].values():\n",
        "    for chapter in book.values():\n",
        "        for verse in chapter.values():\n",
        "            for qa in verse:\n",
        "                questions.append(qa['question'])\n",
        "\n",
        "# המרת כל השאלות ל-embeddings\n",
        "embeddings = embeddingVectors(questions).numpy()\n",
        "\n",
        "# בניית אינדקס FAISS\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# פונקציה למציאת השאלות הקרובות ביותר\n",
        "def find_nearest_questions(query, k=5):\n",
        "    query_embedding = embeddingVectors([query]).numpy()\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
        "    return nearest_questions\n",
        "\n",
        "# פונקציה לרנקר (הדוגמה היא פשוטה. ניתן לשפר בהתאם לצורך)\n",
        "def rerank_documents1(query, documents, top_n=5):\n",
        "    # המרת השאלה ל-embedding\n",
        "    query_embedding = embeddingVectors([query]).numpy()\n",
        "\n",
        "    # המרת המסמכים ל-embeddings\n",
        "    doc_embeddings = embeddingVectors(documents).numpy()\n",
        "\n",
        "    # חישוב המרחקים\n",
        "    distances = np.linalg.norm(doc_embeddings - query_embedding, axis=1)\n",
        "\n",
        "    # מציאת האינדקסים של המסמכים הקרובים ביותר\n",
        "    nearest_indices = distances.argsort()[:top_n]\n",
        "\n",
        "    # החזרת המסמכים המסודרים לפי הקרבה\n",
        "    reranked_documents = [documents[idx] for idx in nearest_indices]\n",
        "    return reranked_documents\n",
        "\n",
        "def rerank_documents(query, docs, top_n):\n",
        "    results = co.rerank(model=\"rerank-multilingual-v3.0\", query=query, documents=docs, top_n=top_n, return_documents=True)\n",
        "    reranked_docs = [res.document.text for res in results.results]\n",
        "    return reranked_docs\n",
        "\n",
        "# שאלה לדוגמה\n",
        "query = \"האם נשים חיבות בתפילה\"\n",
        "\n",
        "# מציאת השאלות הקרובות ביותר\n",
        "nearest_questions = find_nearest_questions(query, k=10)\n",
        "\n",
        "# רנקר על התוצאות הקרובות ביותר\n",
        "reranked_questions = rerank_documents(query, nearest_questions, top_n=5)\n",
        "\n",
        "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
        "for q in reranked_questions:\n",
        "    print(q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "4-Tdt5Ui3eBA",
        "outputId": "0d521042-440d-47fe-8c73-4afd1f6cfa9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'co' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f4ea8b57d775>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# רנקר על התוצאות הקרובות ביותר\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mreranked_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearest_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"השאלות הקרובות ביותר לשאלה שלך הן:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f4ea8b57d775>\u001b[0m in \u001b[0;36mrerank_documents\u001b[0;34m(query, docs, top_n)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rerank-multilingual-v3.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mreranked_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreranked_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'co' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wUE0a6D3fago"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import cohere\n",
        "\n",
        "# הגדרת המודל והטוקנייזר של heBERT\n",
        "model_name = \"avichr/heBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# פונקציה ליצירת embeddings\n",
        "def embeddingVectors(text):\n",
        "    encoding = tokenizer.batch_encode_plus(\n",
        "        text,  # List of input texts\n",
        "        padding=True,  # Pad to the maximum sequence length\n",
        "        truncation=True,  # Truncate to the maximum sequence length if necessary\n",
        "        return_tensors='pt',  # Return PyTorch tensors\n",
        "        add_special_tokens=True  # Add special tokens CLS and SEP\n",
        "    )\n",
        "    input_ids = encoding['input_ids']  # Token IDs\n",
        "    attention_mask = encoding['attention_mask']  # Attention mask\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        word_embeddings = outputs.last_hidden_state  # (batch_size, sequence_length, embedding_dim)\n",
        "        # החזרת ה-embedding של ה-CLS token\n",
        "        sentence_embedding = word_embeddings[:, 0, :]  # בחירת האיבר הראשון בכל רצף (CLS token)\n",
        "    return sentence_embedding  # This contains the embeddings\n",
        "\n",
        "# קריאת הקובץ JSON\n",
        "with open('/content/sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# שליפת כל השאלות מהקובץ\n",
        "questions = []\n",
        "for book in data['contain'].values():\n",
        "    for chapter in book.values():\n",
        "        for verse in chapter.values():\n",
        "            for qa in verse:\n",
        "                questions.append(qa['question'])\n",
        "\n",
        "# המרת כל השאלות ל-embeddings\n",
        "embeddings = embeddingVectors(questions).cpu().numpy()\n",
        "\n",
        "# בניית אינדקס FAISS\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# פונקציה למציאת השאלות הקרובות ביותר\n",
        "def find_nearest_questions(query, k=20):\n",
        "    query_embedding = embeddingVectors([query]).cpu().numpy()\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
        "    return nearest_questions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "rNI-4taCTw-T",
        "outputId": "024da15e-1e01-4991-e0f2-0f6d8cdc7a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/פרי גני שוע.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6ac7f9f7a281>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# קריאת הקובץ JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/פרי גני שוע.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/פרי גני שוע.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# הגדרת מפתח ה-API של Co:here\n",
        "\n",
        "co = cohere.Client('IEo7xZnWi8cPD7jzR2dpsw3cvdGjchG6RCKPko05')\n",
        "# פונקציה לרנקר\n",
        "def rerank_documents(query, docs, top_n):\n",
        "    results = co.rerank(model=\"rerank-multilingual-v3.0\", query=query, documents=docs, top_n=top_n, return_documents=True)\n",
        "    reranked_docs = [res.document.text for res in results.results]\n",
        "    return reranked_docs\n",
        "\n",
        "# שאלה לדוגמה\n",
        "query = \"האם נשים חיבות בתפילה\"\n",
        "\n",
        "# מציאת השאלות הקרובות ביותר\n",
        "nearest_questions = find_nearest_questions(query, k=20)\n",
        "\n",
        "# רנקר על התוצאות הקרובות ביותר\n",
        "reranked_questions = rerank_documents(query, nearest_questions, top_n=5)\n",
        "\n",
        "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
        "for q in reranked_questions:\n",
        "    print(q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOq_KdI8ZU7M",
        "outputId": "b27c2268-9b4d-4965-d522-2923c2847fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "השאלות הקרובות ביותר לשאלה שלך הן:\n",
            "1. מדוע הנשים חייבות בברכת התורה?\n",
            "5. האם נשים חייבות בברכות השחר?\n",
            "3. האם יש חיוב מאה ברכות על נשים וקטנים?\n",
            "1. האם נשים מוציאות האנשים בברכת המזון?\n",
            "2. האם הנשים יכולות להוציא את האנשים בברכת התורה? וקטן?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRyCsrh40h3k"
      },
      "outputs": [],
      "source": [
        "# import cohere\n",
        "# import faiss\n",
        "# import numpy as np\n",
        "# from langchain.embeddings import BedrockEmbeddings  # וודא שהחבילה מותקנת\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings  # שימוש באפשרות אחרת אם BedrockEmbeddings לא עובד\n",
        "\n",
        "# # הגדרת המפתח של Coherence\n",
        "# COHERE_API_KEY = 'YOUR_COHERE_API_KEY'\n",
        "# co = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "# # רשימת השאלות\n",
        "# questions = [\n",
        "#     \"מה היא מצוות תפילין?\",\n",
        "#     \"כיצד יש להתפלל במניין?\",\n",
        "#     \"מהי מצוות שמירת השבת?\",\n",
        "#     \"כיצד יש לקיים את מצוות נטילת לולב?\",\n",
        "#     \"מה הם דיני כשרות?\"\n",
        "# ]\n",
        "\n",
        "# # המרת השאלות ל-embeddings\n",
        "# def get_embeddings(questions):\n",
        "#     response = co.embed(texts=questions)\n",
        "#     embeddings = response.embeddings\n",
        "#     return np.array(embeddings)\n",
        "\n",
        "# embeddings = get_embeddings(questions)\n",
        "\n",
        "# # בניית אינדקס FAISS\n",
        "# dimension = embeddings.shape[1]\n",
        "# index = faiss.IndexFlatL2(dimension)\n",
        "# index.add(embeddings)\n",
        "\n",
        "# # פונקציה למציאת השאלות הקרובות ביותר\n",
        "# def find_nearest_questions(query, k=5):\n",
        "#     query_embedding = get_embeddings([query])[0].reshape(1, -1)\n",
        "#     distances, indices = index.search(query_embedding, k)\n",
        "#     nearest_questions = [questions[idx] for idx in indices[0]]\n",
        "#     return nearest_questions\n",
        "\n",
        "# # שאלה לדוגמה\n",
        "# query = \"מה דיני תפילה במניין?\"\n",
        "\n",
        "# # מציאת השאלות הקרובות ביותר\n",
        "# nearest_questions = find_nearest_questions(query, k=5)\n",
        "# print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
        "# for q in nearest_questions:\n",
        "#     print(q)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyrL2dKC0jT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}