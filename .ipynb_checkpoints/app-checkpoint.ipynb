{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "4POBCmgN0kAs",
    "outputId": "df008e33-ac24-47b8-e248-f9deda26f5d8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cohere in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.6.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (1.34.145)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (1.9.5)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (0.19.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (2.32.0.20240712)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cohere) (4.12.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.145 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.34.145)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (0.10.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.21.2->cohere) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.9.2->cohere) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers<1,>=0.15->cohere) (0.24.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.35.0,>=1.34.145->boto3<2.0.0,>=1.34.0->cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.145->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.4.6)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.22 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.22)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.93)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (24.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.22->langchain) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere faiss-cpu\n",
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "YXHODrjl05eu",
    "outputId": "9a880c76-c5ba-4c23-898b-9bd71d5ffba3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "print(\"fff\")\n",
    "# הגדרת המודל והטוקנייזר של heBERT\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# פונקציה ליצירת embeddings לשאלה נתונה\n",
    "def get_embedding(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # אנו לוקחים את הממוצע של כל ה-embeddings של הטוקנים\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "YXHODrjl05eu",
    "outputId": "9a880c76-c5ba-4c23-898b-9bd71d5ffba3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "# הגדרת המודל והטוקנייזר של heBERT\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "YXHODrjl05eu",
    "outputId": "9a880c76-c5ba-4c23-898b-9bd71d5ffba3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# פונקציה ליצירת embeddings לשאלה נתונה\n",
    "#הגדרת max_length: הוספתי את הפרמטר max_length=512 בקריאה ל-tokenizer כדי למנוע את הודעת האזהרה.\n",
    "def get_embedding(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # אנו לוקחים את הממוצע של כל ה-embeddings של הטוקנים\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "-DmnOq_vcqQl",
    "outputId": "ffb252d1-eded-4d39-d7ba-efb742375351",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#from google.colab import drive\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#drive.mount('/content/drive')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# קריאת הקובץ JSON\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sample_data/פרי גני שוע.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# שליפת כל השאלות מהקובץ\u001b[39;00m\n\u001b[0;32m      8\u001b[0m questions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# קריאת הקובץ JSON\n",
    "with open('./sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# שליפת כל השאלות מהקובץ\n",
    "questions = []\n",
    "for book in data['contain'].values():\n",
    "    for chapter in book.values():\n",
    "        for verse in chapter.values():\n",
    "            for qa in verse:\n",
    "                questions.append(qa['question'])\n",
    "\n",
    "# המרת כל השאלות ל-embeddings\n",
    "embeddings = np.array([get_embedding(question) for question in questions])\n",
    "print(\"cancel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BZrDfx4aWBg"
   },
   "outputs": [],
   "source": [
    "# בניית אינדקס FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# פונקציה למציאת השאלות הקרובות ביותר\n",
    "def find_nearest_questions(query, k=5):\n",
    "    query_embedding = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
    "    return nearest_questions\n",
    "print(\"cancel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLQxxhyiagLQ"
   },
   "outputs": [],
   "source": [
    "# שאלה לדוגמה\n",
    "query = \"האם נשים חיבות בתפילה\"\n",
    "\n",
    "# מציאת השאלות הקרובות ביותר\n",
    "nearest_questions = find_nearest_questions(query, k=5)\n",
    "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
    "for q in nearest_questions:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEPXf9zy0QgK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "4-Tdt5Ui3eBA",
    "outputId": "0d521042-440d-47fe-8c73-4afd1f6cfa9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'co' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f4ea8b57d775>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# רנקר על התוצאות הקרובות ביותר\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mreranked_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearest_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"השאלות הקרובות ביותר לשאלה שלך הן:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f4ea8b57d775>\u001b[0m in \u001b[0;36mrerank_documents\u001b[0;34m(query, docs, top_n)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rerank-multilingual-v3.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mreranked_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreranked_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'co' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "# הגדרת המודל והטוקנייזר של heBERT\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# פונקציה ליצירת embeddings\n",
    "def embeddingVectors(text):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        text,  # List of input texts\n",
    "        padding=True,  # Pad to the maximum sequence length\n",
    "        truncation=True,  # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        add_special_tokens=True  # Add special tokens CLS and SEP\n",
    "    )\n",
    "    input_ids = encoding['input_ids']  # Token IDs\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state  # (batch_size, sequence_length, embedding_dim)\n",
    "        # החזרת ה-embedding של ה-CLS token\n",
    "        sentence_embedding = word_embeddings[:, 0, :]  # בחירת האיבר הראשון בכל רצף (CLS token)\n",
    "    return sentence_embedding  # This contains the embeddings\n",
    "\n",
    "# קריאת הקובץ JSON\n",
    "with open('/content/sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# שליפת כל השאלות מהקובץ\n",
    "questions = []\n",
    "for book in data['contain'].values():\n",
    "    for chapter in book.values():\n",
    "        for verse in chapter.values():\n",
    "            for qa in verse:\n",
    "                questions.append(qa['question'])\n",
    "\n",
    "# המרת כל השאלות ל-embeddings\n",
    "embeddings = embeddingVectors(questions).numpy()\n",
    "\n",
    "# בניית אינדקס FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# פונקציה למציאת השאלות הקרובות ביותר\n",
    "def find_nearest_questions(query, k=5):\n",
    "    query_embedding = embeddingVectors([query]).numpy()\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
    "    return nearest_questions\n",
    "\n",
    "# פונקציה לרנקר (הדוגמה היא פשוטה. ניתן לשפר בהתאם לצורך)\n",
    "def rerank_documents1(query, documents, top_n=5):\n",
    "    # המרת השאלה ל-embedding\n",
    "    query_embedding = embeddingVectors([query]).numpy()\n",
    "\n",
    "    # המרת המסמכים ל-embeddings\n",
    "    doc_embeddings = embeddingVectors(documents).numpy()\n",
    "\n",
    "    # חישוב המרחקים\n",
    "    distances = np.linalg.norm(doc_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # מציאת האינדקסים של המסמכים הקרובים ביותר\n",
    "    nearest_indices = distances.argsort()[:top_n]\n",
    "\n",
    "    # החזרת המסמכים המסודרים לפי הקרבה\n",
    "    reranked_documents = [documents[idx] for idx in nearest_indices]\n",
    "    return reranked_documents\n",
    "\n",
    "def rerank_documents(query, docs, top_n):\n",
    "    results = co.rerank(model=\"rerank-multilingual-v3.0\", query=query, documents=docs, top_n=top_n, return_documents=True)\n",
    "    reranked_docs = [res.document.text for res in results.results]\n",
    "    return reranked_docs\n",
    "\n",
    "# שאלה לדוגמה\n",
    "query = \"האם נשים חיבות בתפילה\"\n",
    "\n",
    "# מציאת השאלות הקרובות ביותר\n",
    "nearest_questions = find_nearest_questions(query, k=10)\n",
    "\n",
    "# רנקר על התוצאות הקרובות ביותר\n",
    "reranked_questions = rerank_documents(query, nearest_questions, top_n=5)\n",
    "\n",
    "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
    "for q in reranked_questions:\n",
    "    print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUE0a6D3fago"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "rNI-4taCTw-T",
    "outputId": "024da15e-1e01-4991-e0f2-0f6d8cdc7a1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/sample_data/פרי גני שוע.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6ac7f9f7a281>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# קריאת הקובץ JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/פרי גני שוע.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/פרי גני שוע.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import cohere\n",
    "\n",
    "# הגדרת המודל והטוקנייזר של heBERT\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# פונקציה ליצירת embeddings\n",
    "def embeddingVectors(text):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        text,  # List of input texts\n",
    "        padding=True,  # Pad to the maximum sequence length\n",
    "        truncation=True,  # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        add_special_tokens=True  # Add special tokens CLS and SEP\n",
    "    )\n",
    "    input_ids = encoding['input_ids']  # Token IDs\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state  # (batch_size, sequence_length, embedding_dim)\n",
    "        # החזרת ה-embedding של ה-CLS token\n",
    "        sentence_embedding = word_embeddings[:, 0, :]  # בחירת האיבר הראשון בכל רצף (CLS token)\n",
    "    return sentence_embedding  # This contains the embeddings\n",
    "\n",
    "# קריאת הקובץ JSON\n",
    "with open('/content/sample_data/פרי גני שוע.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# שליפת כל השאלות מהקובץ\n",
    "questions = []\n",
    "for book in data['contain'].values():\n",
    "    for chapter in book.values():\n",
    "        for verse in chapter.values():\n",
    "            for qa in verse:\n",
    "                questions.append(qa['question'])\n",
    "\n",
    "# המרת כל השאלות ל-embeddings\n",
    "embeddings = embeddingVectors(questions).cpu().numpy()\n",
    "\n",
    "# בניית אינדקס FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# פונקציה למציאת השאלות הקרובות ביותר\n",
    "def find_nearest_questions(query, k=20):\n",
    "    query_embedding = embeddingVectors([query]).cpu().numpy()\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    nearest_questions = [questions[idx] for idx in indices[0]]\n",
    "    return nearest_questions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOq_KdI8ZU7M",
    "outputId": "b27c2268-9b4d-4965-d522-2923c2847fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "השאלות הקרובות ביותר לשאלה שלך הן:\n",
      "1. מדוע הנשים חייבות בברכת התורה?\n",
      "5. האם נשים חייבות בברכות השחר?\n",
      "3. האם יש חיוב מאה ברכות על נשים וקטנים?\n",
      "1. האם נשים מוציאות האנשים בברכת המזון?\n",
      "2. האם הנשים יכולות להוציא את האנשים בברכת התורה? וקטן?\n"
     ]
    }
   ],
   "source": [
    "# הגדרת מפתח ה-API של Co:here\n",
    "\n",
    "co = cohere.Client('IEo7xZnWi8cPD7jzR2dpsw3cvdGjchG6RCKPko05')\n",
    "# פונקציה לרנקר\n",
    "def rerank_documents(query, docs, top_n):\n",
    "    results = co.rerank(model=\"rerank-multilingual-v3.0\", query=query, documents=docs, top_n=top_n, return_documents=True)\n",
    "    reranked_docs = [res.document.text for res in results.results]\n",
    "    return reranked_docs\n",
    "\n",
    "# שאלה לדוגמה\n",
    "query = \"האם נשים חיבות בתפילה\"\n",
    "\n",
    "# מציאת השאלות הקרובות ביותר\n",
    "nearest_questions = find_nearest_questions(query, k=20)\n",
    "\n",
    "# רנקר על התוצאות הקרובות ביותר\n",
    "reranked_questions = rerank_documents(query, nearest_questions, top_n=5)\n",
    "\n",
    "print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
    "for q in reranked_questions:\n",
    "    print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRyCsrh40h3k"
   },
   "outputs": [],
   "source": [
    "# import cohere\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "# from langchain.embeddings import BedrockEmbeddings  # וודא שהחבילה מותקנת\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings  # שימוש באפשרות אחרת אם BedrockEmbeddings לא עובד\n",
    "\n",
    "# # הגדרת המפתח של Coherence\n",
    "# COHERE_API_KEY = 'YOUR_COHERE_API_KEY'\n",
    "# co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "# # רשימת השאלות\n",
    "# questions = [\n",
    "#     \"מה היא מצוות תפילין?\",\n",
    "#     \"כיצד יש להתפלל במניין?\",\n",
    "#     \"מהי מצוות שמירת השבת?\",\n",
    "#     \"כיצד יש לקיים את מצוות נטילת לולב?\",\n",
    "#     \"מה הם דיני כשרות?\"\n",
    "# ]\n",
    "\n",
    "# # המרת השאלות ל-embeddings\n",
    "# def get_embeddings(questions):\n",
    "#     response = co.embed(texts=questions)\n",
    "#     embeddings = response.embeddings\n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# embeddings = get_embeddings(questions)\n",
    "\n",
    "# # בניית אינדקס FAISS\n",
    "# dimension = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatL2(dimension)\n",
    "# index.add(embeddings)\n",
    "\n",
    "# # פונקציה למציאת השאלות הקרובות ביותר\n",
    "# def find_nearest_questions(query, k=5):\n",
    "#     query_embedding = get_embeddings([query])[0].reshape(1, -1)\n",
    "#     distances, indices = index.search(query_embedding, k)\n",
    "#     nearest_questions = [questions[idx] for idx in indices[0]]\n",
    "#     return nearest_questions\n",
    "\n",
    "# # שאלה לדוגמה\n",
    "# query = \"מה דיני תפילה במניין?\"\n",
    "\n",
    "# # מציאת השאלות הקרובות ביותר\n",
    "# nearest_questions = find_nearest_questions(query, k=5)\n",
    "# print(\"השאלות הקרובות ביותר לשאלה שלך הן:\")\n",
    "# for q in nearest_questions:\n",
    "#     print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyrL2dKC0jT-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
